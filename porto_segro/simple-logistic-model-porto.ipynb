{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eaaa1387-3695-4e36-9996-33e63e672117",
    "_uuid": "9103edb87e1990e85fbb467d699c41ac109f50f4"
   },
   "source": [
    "## Porto Seguro’s Safe Driver Prediction\n",
    "\n",
    "<br><font color=blue>The aim of this compitation is to predict probability that a driver will intiate an auto insurance claim next year.A more accurate prediction will allow them to further tailor their prices, and hopefully make auto insurance coverage more accessible to more drivers. </font>\n",
    "\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. [Read data set](#Read-data-set)\n",
    "2. [Explore data set](#Explore-data-set)\n",
    "3. [Correlation plot](#Correlation-plot)\n",
    "4. [Missing value is data set](#Missing-value-is-data-set)\n",
    "5. [Convert variables into category type](#Convert-variables-into-category-type)\n",
    "6. [Univariate analysis](#Univariate-analysis)\n",
    "7. [Descrictive Statistic Features](#Descrictive-Statistic-Features)\n",
    "8. [Determine outliers in dataset](#Determine-outliers-in-dataset)\n",
    "9. [One Hot Encoding](#One-Hot-Encoding)\n",
    "10. [Split data set](#Split-data-set)\n",
    "11. [Hyperparameter tuning](#Hyperparameter-tuning)\n",
    "12. [Logistic Regression model](#Logistic-Regression-model)\n",
    "13. [Model performance](#Model-performance)\n",
    "14. [Reciever Operating Charactaristics](#Reciever-Operating-Charactaristics)\n",
    "15. [Predict for unseen data set](#Predict-for-unseen-data-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "!pip install lightgbm\n",
    "!pip install missingno\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "702a9e71-6183-4c0b-80a2-c61151190d2d",
    "_uuid": "a6c78f4cd0d8cec2854073836ec65acf05376871"
   },
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "844ee59d-9156-4e39-ba4b-228ec74a5d52",
    "_uuid": "444c4c4ed80511192a47ce2646effaf4c4f82f15",
    "execution": {
     "iopub.execute_input": "2023-01-09T06:02:53.983382Z",
     "iopub.status.busy": "2023-01-09T06:02:53.983098Z",
     "iopub.status.idle": "2023-01-09T06:02:53.993013Z",
     "shell.execute_reply": "2023-01-09T06:02:53.992245Z",
     "shell.execute_reply.started": "2023-01-09T06:02:53.983330Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import lightgbm as lgbm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\n",
    "from sklearn.model_selection import StratifiedKFold,GridSearchCV\n",
    "import missingno as mssno\n",
    "import xgboost as xgb\n",
    "seed =45\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"LGBM\":lgbm.LGBMClassifier(n_estimators=600, objective='binary' ),\n",
    "     \"LOGI\":LogisticRegression()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=MODELS[\"LGBM\"]\n",
    "# One-Hot処理の有無\n",
    "IS_OHE=True\n",
    "# 相関係数がゼロであるps_calcをドロップするか\n",
    "IS_DROP_COEFF_ZERO=True\n",
    "# 欠損値が存在するという情報をを加えるか\n",
    "IS_ADD_NULL_VAL_INFO=True\n",
    "# アウトライヤーを1%/99%値で置き換えるか\n",
    "IS_OUTLIER_ROUNDING=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e3a43d43-3b85-4a3b-8afa-1b8711c75235",
    "_uuid": "724c2f9405e719a89ed73104600c81cf6a16ea7d"
   },
   "source": [
    "## Read data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8d8ee6aa-69a2-42d4-a9a4-8cd05d26f903",
    "_uuid": "c0986ac74bc4ec65d6094931b51a98abc215587a",
    "execution": {
     "iopub.execute_input": "2023-01-09T06:02:56.791608Z",
     "iopub.status.busy": "2023-01-09T06:02:56.791318Z",
     "iopub.status.idle": "2023-01-09T06:03:07.190464Z",
     "shell.execute_reply": "2023-01-09T06:03:07.189596Z",
     "shell.execute_reply.started": "2023-01-09T06:02:56.791557Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '../input/'\n",
    "#path = 'dataset/'\n",
    "train = pd.read_csv(path+'train.csv',na_values=-1)\n",
    "test = pd.read_csv(path+'test.csv',na_values=-1)\n",
    "print('Number rows and columns:',train.shape)\n",
    "print('Number rows and columns:',test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "623bd609-2b4f-4494-b1cc-6789aec6c6b8",
    "_uuid": "4a3979d6599c6f2c0b752d3c7ff67e84eca6924a"
   },
   "source": [
    "Porto Seguro provided close to 600k and 900k observation of train and test dataset respectively. They were 57 feature anonymized in order to protect company trade secrets, but they were given bit informaation about  The train and test data set contains feature with similar grouping are tagged with (e.g., ind, reg, car, cat, calc, bin). Values of  -1 indicate that the feature was missing from the observation.\n",
    "\n",
    "Porto Seguroはそれぞれ列車と試験データセットの600 kと900 kに近い観測を提供した。これらは、企業の企業秘密を保護するために匿名化された57の特徴であったが、列車とテストのデータセットには、同様のグループ化が(例:ind, reg, car, cat, calc, bin)のタグが付けられた特徴が含まれていることに関するビット情報が与えられた。-1の値は、その特徴が観測から欠落していたことを示します。\n",
    "\n",
    "## Explore data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "274bec19-7cc4-4dd5-b7c7-717f126a118d",
    "_uuid": "04567639b2846fa740ff1a54ac3fcb74edea6df8"
   },
   "outputs": [],
   "source": [
    "train.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "98256efd-0482-47ce-854f-dd445a23e2a7",
    "_uuid": "edd54a9dda569f65e87cd8fb76aefee967609a90"
   },
   "source": [
    "## Target varaiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3b75a9be-6f27-4d0b-9323-7ca92daa735d",
    "_uuid": "adae6789bdc30867828cbdf6c50ef94a0078f33e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "sns.countplot(train['target'],palette='rainbow')\n",
    "plt.xlabel('Target')\n",
    "\n",
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "41422bae-87b5-4daa-9899-46b14b10ad7e",
    "_uuid": "a2c533082789de2d60c6f5bcbaa45b49e09822bb"
   },
   "source": [
    "The 'target' variable in imbalanced. The target column in data set is whether or not claim was filed for that policy holder. The target variable is quite unbalanced, with only  %4 of  policyholders in training data filing claim within the year.\n",
    "\n",
    "'target'変数が不均衡です。データセットのターゲット列は、そのポリシー保持者に対してクレームが提出されたかどうかです。目標変数は非常に不均衡であり、トレーニングデータファイリングの契約者のうち年内に請求するのは%4人のみである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7347c658-4303-4562-9a8e-144caf88b80f",
    "_uuid": "a27bb963e89444341fccefe70894ca79717aa39c"
   },
   "source": [
    "## Correlation plot\n",
    "Correlation is a measure bivariate analysis that measure the strength of assciation between variable and direction of relationship.In terms of strength of relationship, the value of the correlation coefficient varies between +1 and -1\n",
    "\n",
    "相関は、変数と関係の方向の間の関連の強さを測定する測度二変量解析である。相関係数の値は、相関の強さでは+1から-1の間で変化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "45e9664f-9610-4caf-8a9b-d79f6c595b82",
    "_uuid": "a52eb3bd60ed40a79ed84ed9c081c9b458da2d36"
   },
   "outputs": [],
   "source": [
    "cor = train.drop('id',axis=1).corr()\n",
    "plt.figure(figsize=(16,16))\n",
    "sns.heatmap(cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c152f9e7-9486-4b74-816d-56807e5029ac",
    "_uuid": "3a02951d48681ac1bcba928096aa2857bcacfda3"
   },
   "source": [
    "> The correlation coefficient for **ps_calc** is 0,so we will drop these from our dataset.\n",
    "\n",
    "> **ps_calc**の相関係数は0であるため、データセットからこれらを削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "be343a8b-8128-43f3-871f-9ab2a1fa9ea1",
    "_uuid": "16cff6ebc13b3f747b9439da7341a8bb59654b01"
   },
   "outputs": [],
   "source": [
    "ps_cal = train.columns[train.columns.str.startswith('ps_calc')]\n",
    "if IS_DROP_COEFF_ZERO:\n",
    "    train = train.drop(ps_cal,axis =1)\n",
    "    test = test.drop(ps_cal,axis=1)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f34156a3-5e99-4384-9915-8281ef37c7ea",
    "_uuid": "f1e92760c76f66d0a652c44c4fd771d2ff4d342c"
   },
   "source": [
    "## Missing value is data set\n",
    ">Values of -1 indicate that the feature was missing from the observation. The target columns signifies whether or not a claim was filed for that policy holder.\n",
    ">-1の値は、その特徴が観測から欠落していたことを示します。ターゲット列は、そのポリシー保持者に対してクレームが提出されたかどうかを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c4a3985f-d5a2-464d-bb72-f86305559bb6",
    "_uuid": "3acab507913fc33094a7d31965fd46a67ce84892"
   },
   "outputs": [],
   "source": [
    "k= pd.DataFrame()\n",
    "k['train']= train.isnull().sum()\n",
    "k['test'] = test.isnull().sum()\n",
    "fig,ax = plt.subplots(figsize=(16,5))\n",
    "k.plot(kind='bar',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows_with_missing_values(df):\n",
    "    \"\"\"\n",
    "    データフレーム内の欠損値を含む行の数をカウントする関数\n",
    "    :param df: pandas.DataFrame, 入力データフレーム\n",
    "    :return: int, 欠損値を含む行の数\n",
    "    \"\"\"\n",
    "    return df.isnull().any(axis=1).sum()\n",
    "def count_rows_with_nulls_in_target1(df):\n",
    "    # targetが1で、欠損値を1つ以上含むカラムを持つ行の数を取得する\n",
    "    target1_rows = df[df['target'] == 1]\n",
    "    num_rows_target1_with_nulls = target1_rows.isnull().any(axis=1).sum()\n",
    "    return num_rows_target1_with_nulls\n",
    "\n",
    "def count_rows_without_nulls_in_target0(df):\n",
    "    # targetが0で、全てのカラムに欠損値が含まれない行の数を取得する\n",
    "    target0_rows = df[df['target'] == 0]\n",
    "    num_rows_target0_without_nulls = target0_rows.notnull().all(axis=1).sum()\n",
    "    return num_rows_target0_without_nulls\n",
    "\n",
    "num_rows_with_missing_values = count_rows_with_missing_values(train)\n",
    "print(\"データ総数：\",train.shape[0])\n",
    "print('欠損値を含む行の数:', num_rows_with_missing_values)\n",
    "num_rows_target1 = (train['target'] == 1).sum()\n",
    "print('保険請求した件数:', num_rows_target1)\n",
    "\n",
    "# count_rows_with_nulls_in_target1関数を使って、targetが1で欠損値を1つ以上含む行数をカウントする\n",
    "num_rows_target1_with_nulls = count_rows_with_nulls_in_target1(train)\n",
    "print('targetが1で、欠損値を1つ以上含むカラムを持つ行の数:', num_rows_target1_with_nulls)\n",
    "\n",
    "\n",
    "# count_rows_without_nulls_in_target0関数を使って、targetが0で欠損値を含まない行数をカウントする\n",
    "num_rows_target0_without_nulls = count_rows_without_nulls_in_target0(train)\n",
    "print('targetが0で、欠損値を含まない行の数:', num_rows_target0_without_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欠損値が１以上あるという情報を加える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_null_val_flag(df=None,cols_with_missing=None):\n",
    "    # 欠損値を含む列を選択\n",
    "    if cols_with_missing is None:\n",
    "        cols_with_missing = df.columns[df.isnull().any()].tolist()\n",
    "    print(cols_with_missing)\n",
    "\n",
    "    # 欠損値があることを示すフラグ列を作成\n",
    "    for col in cols_with_missing:\n",
    "        df[col + '_missing'] = df[col].isnull().astype(int)\n",
    "\n",
    "    return df,cols_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_ADD_NULL_VAL_INFO:\n",
    "    train,train_cols_with_missing =add_null_val_flag(train,None)\n",
    "    test,_=add_null_val_flag(test,train_cols_with_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2b122697-23ae-4b71-ab79-a45e9c0fb6de",
    "_uuid": "7c560e7b68fc6aeaf879af2f679130fba4bc3cb1"
   },
   "source": [
    "Missing value in test train data set are in same propotion and same column\n",
    "\n",
    "テスト列車のデータセットの欠損値は、同じ比率と同じ列にある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a2971ff3-f3c2-429b-9de8-5303c1628a55",
    "_uuid": "d2ab0f29e3a714ef1c8ecb257d72f8c374565c25"
   },
   "outputs": [],
   "source": [
    "mssno.bar(train,color='y',figsize=(16,4),fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cbefe505-896e-40d0-9296-0acb17b4f0e1",
    "_uuid": "a2e787a7e4c1ba5b7641566b9637a1c69785c361"
   },
   "outputs": [],
   "source": [
    "mssno.bar(test,color='b',figsize=(16,4),fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b521dab6-97aa-43ee-9bdd-5652de87e27d",
    "_uuid": "2c727b1cc35bd3f7c66ce1b836d360483a90f48c"
   },
   "outputs": [],
   "source": [
    "mssno.matrix(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07bd47a2d4b572565537131b721dc70f73f627c5"
   },
   "source": [
    "### Replace missing value with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b4c24c09-e291-4284-9c1d-cf6e13d2f0d6",
    "_uuid": "a720df58580d19efd53f4cf425f742345a07c283",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def missing_value(df):\n",
    "    col = df.columns\n",
    "    # 最頻値による欠損地補完\n",
    "    for i in col:\n",
    "        if df[i].isnull().sum()>0:\n",
    "            df[i].fillna(df[i].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9c39b3ec-8164-4c55-96e4-b4b86146c227",
    "_uuid": "1d39c0068c330de2e7d9a48a20b384875c2a6080",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "missing_value(train)\n",
    "missing_value(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "04bab9cb-582e-47f2-a6c8-67f88215a190",
    "_uuid": "d3246ac4dadc1ff0123e359fdc4dade2f1e5a2e8"
   },
   "source": [
    "## Convert variables into category type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a82f63c7-0b4c-4d95-abd7-5ad822fbfd7d",
    "_uuid": "c255a7b1cce95493d7ac8975982dc6481c771683"
   },
   "outputs": [],
   "source": [
    "def basic_details(df):\n",
    "    b = pd.DataFrame()\n",
    "    b['Missing value'] = df.isnull().sum()\n",
    "    b['N unique value'] = df.nunique()\n",
    "    b['dtype'] = df.dtypes\n",
    "    return b\n",
    "basic_details(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4b9c1309-a779-487c-b8c3-cd32cfdf834a",
    "_uuid": "20bacaf06dfa681cd1d96c24232db50c9d74e7cb"
   },
   "source": [
    ">The unique value of \"ps_car_11_cat\" is maximum in the data set is 104"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カテゴリ変数に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f1ec6717-3037-42ba-99d5-bd5a427ce8be",
    "_uuid": "31936a277456a41fc9a11c30b3c07ad9ec092ed9",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def category_type(df):\n",
    "    col = df.columns\n",
    "    for i in col:\n",
    "        if df[i].nunique()<=104:\n",
    "            #print(df[i].nunique())\n",
    "            # print(df[i].astype(\"category\"))\n",
    "            df[i] = df[i].astype('category')\n",
    "category_type(train)\n",
    "category_type(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fab77cbc-8d08-4d85-93f0-a2e19c1ff328",
    "_uuid": "aa3e77055fe42cd27a4058fe5894d356cefd9f58"
   },
   "source": [
    "## Descrictive Statistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "252f98da-1994-4ba3-94ea-a9fc9aed449a",
    "_uuid": "4ec0d5b1a70185214cf83d5317c5ec2b06bc85bb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def descrictive_stat_feat(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    dcol= [c for c in train.columns if train[c].nunique()>=10]\n",
    "    dcol.remove('id')   \n",
    "    d_median = df[dcol].median(axis=0)\n",
    "    d_mean = df[dcol].mean(axis=0)\n",
    "    q1 = df[dcol].apply(np.float32).quantile(0.25)\n",
    "    q3 = df[dcol].apply(np.float32).quantile(0.75)\n",
    "    print(d_mean)\n",
    "    print(dcol)\n",
    "    #Add mean and median column to data set having more then 10 categories\n",
    "    for c in dcol:\n",
    "        df[c+str('_q1')] = (df[c].astype(np.float32).values < q1[c]).astype(np.int8)\n",
    "        df[c+str('_q3')] = (df[c].astype(np.float32).values > q3[c]).astype(np.int8)\n",
    "        df[c+str('_mean_range')] = (df[c].astype(np.float32).values > d_mean[c]).astype(np.int8)\n",
    "        df[c+str('_median_range')] = (df[c].astype(np.float32).values > d_median[c]).astype(np.int8)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_col = [col for col in train.columns if 'bin' in col]\n",
    "print(bin_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = [col for col in train.columns if '_cat' in col]\n",
    "print(cat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cat_col = list(train.select_dtypes(include=['category']).columns)\n",
    "\n",
    "other_cat_col = [c for c in tot_cat_col if c not in cat_col+ bin_col]\n",
    "other_cat_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "de22e1b0-ba8a-4034-b8cc-2b5ef74ec8d8",
    "_uuid": "2d5a89189a46e812f4ee4dd3248e3d8784db76a0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "num_col = [c for c in train.columns if c not in tot_cat_col]\n",
    "num_col.remove('id')\n",
    "num_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cf539b20-199d-4774-b43b-19f8be453067",
    "_uuid": "9a5478ec398de29a7cec592f3758d85f1d8294bc"
   },
   "source": [
    "## Determine outliers in dataset\n",
    "The extreme observations in data set which resembles completely different behavoir from the rest of data point are called outliers. The outliers present in numeric feature are replaced by 1%/99% of feature.\n",
    "\n",
    "データセット内の極端な観測値が、他のデータポイントとはまったく異なる動作に似ている場合は、外れ値と呼ばれます。数値フィーチャーに存在する外れ値は、フィーチャーの1%/99%で置き換えられます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a46dda75-21dd-4d26-9e09-9e849f0b1225",
    "_uuid": "73d6e78fc023492ea66b58b87db91516ea1abf04"
   },
   "outputs": [],
   "source": [
    "def outlier(df,columns):\n",
    "    for i in columns:\n",
    "        quartile_1,quartile_3 = np.percentile(df[i],[25,75])\n",
    "        quartile_f,quartile_l = np.percentile(df[i],[1,99])\n",
    "        IQR = quartile_3-quartile_1\n",
    "        lower_bound = quartile_1 - (1.5*IQR)\n",
    "        upper_bound = quartile_3 + (1.5*IQR)\n",
    "        print(i,lower_bound,upper_bound,quartile_f,quartile_l)\n",
    "                \n",
    "        df[i].loc[df[i] < lower_bound] = quartile_f\n",
    "        df[i].loc[df[i] > upper_bound] = quartile_l\n",
    "\n",
    "if IS_OUTLIER_ROUNDING: \n",
    "    outlier(train,num_col)\n",
    "    outlier(test,num_col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3e1bd0c2-4a49-47a8-980f-4c7e1fe51700",
    "_uuid": "6bc265235a5aefe8eef31ec13d3791d21e5ffae4"
   },
   "source": [
    "## One Hot Encoding\n",
    "A One hot encoding is a representation of categorical variable as binary vectors.It allows the representation of categorical data to be more expresive. This first requires that the categorical values be mapped to integer values, that is label encoding. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n",
    "The Dummy variable trap is a scenario in which the independent variable are multicollinear, a scenario in which two or more variables are highly correlated in simple term one variable can be predicted from the others.\n",
    "\n",
    "One hot encodingはカテゴリ変数をバイナリベクトルとして表現したものである。これにより、カテゴリデータの表現をより表現できるようになります。まず、カテゴリ値を整数値にマッピングする必要があります。次に、各整数値は、1でマークされた整数のインデックスを除き、すべて0の値であるバイナリベクトルとして表されます。\n",
    "ダミー変数トラップは、独立変数が多重共線的であるシナリオであり、2つ以上の変数が単純項で高い相関関係にあるシナリオで、一方の変数を他方から予測することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0f8855e-3ffa-42a3-82c4-f8c52af0476c",
    "_uuid": "e30bc9e1561e242be138b49d88c0927ccc93d790",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def OHE(df1,df2,column):\n",
    "    cat_col = column\n",
    "    #cat_col = df.select_dtypes(include =['category']).columns\n",
    "    len_df1 = df1.shape[0]\n",
    "    \n",
    "    df = pd.concat([df1,df2],ignore_index=True)\n",
    "    c2,c3 = [],{}\n",
    "    \n",
    "    print('Categorical feature',len(column))\n",
    "    for c in cat_col:\n",
    "        if df[c].nunique()>2 :\n",
    "            c2.append(c)\n",
    "            c3[c] = 'ohe_'+c\n",
    "    \n",
    "    df = pd.get_dummies(df, prefix=c3, columns=c2,drop_first=True)\n",
    "\n",
    "    df1 = df.loc[:len_df1-1]\n",
    "    df2 = df.loc[len_df1:]\n",
    "    print('Train',df1.shape)\n",
    "    print('Test',df2.shape)\n",
    "    return df1,df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f9d56861-75ac-4a65-bc37-9349814bb873",
    "_uuid": "fab7439611d6b821d3f2466e990cc241dff9445f"
   },
   "outputs": [],
   "source": [
    "if IS_OHE:\n",
    "    train1,test1 = OHE(train,test,tot_cat_col)\n",
    "else:\n",
    "    train1, test1 = train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "def RescaleData(train, test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(train)\n",
    "    scaler.fit_transform(test)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def DropCalcCol(train, test):\n",
    "    col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "    train = train.drop(col_to_drop, axis=1)\n",
    "    test = test.drop(col_to_drop, axis=1)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne = TSNE(n_components=2, random_state = 0, perplexity = 30, n_iter = 100)\n",
    "    train_2d=tsne.fit_transform(train1)\n",
    "    \n",
    "    plt.scatter(data_2d[:, 0], data_2d[:, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c8ae9692-502d-4106-a69b-bdbea923df0e",
    "_uuid": "ece06f3922f844efaeaf20038fb21294e8549a65"
   },
   "source": [
    "## Split data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4c7ba255-fe2f-4586-87bb-755468f0f444",
    "_uuid": "3668ad7ad71005181596172496e3b2380f2c5301",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = train1.drop(['target','id'],axis=1)\n",
    "y = train1['target'].astype('category')\n",
    "# x_test = test1.drop(['target','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train1['target'].values\n",
    "train_id = train1['id'].values\n",
    "X = train1.drop(['target', 'id'], axis=1)\n",
    "test_id = test1['id']\n",
    "X_test = test1.drop(['id',\"target\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "kf = StratifiedKFold(n_splits=2,random_state=seed,shuffle=True)\n",
    "pred_test_full=0\n",
    "cv_score=[]\n",
    "i=1\n",
    "for train_index,test_index in kf.split(X,y):    \n",
    "    x_train,x_test = X.loc[train_index],X.loc[test_index]\n",
    "    y_train,y_test = y[train_index],y[test_index]\n",
    "        \n",
    "    #criterion can be also : entropy \n",
    "    model = MODEL\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    print('Model Train Score is : ' , model.score(x_train, y_train))\n",
    "    print('Model Test Score is : ' , model.score(x_test, y_test))\n",
    "    \n",
    "    # print('Model Train Gini Score is : ' ,  gini(x_train, y_train))\n",
    "    # print('Model Test Gini Score is : ' ,  gini(x_test, y_test))\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_prob = model.predict_proba(x_test)\n",
    "name = \"model\"\n",
    "joblib.dump(model,name+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=joblib.load(\"model.h5\")\n",
    "#y_pred = model.predict(X_test)\n",
    "y_pred_prob = model.predict_proba(X_test)\n",
    "# print('Predicted Value for RandomForestClassifierModel is : ' , y_pred[:10])\n",
    "print('Prediction Probabilities Value for RandomForestClassifierModel is : ' , y_pred_prob[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit = pd.DataFrame({'id':test['id'],'target':y_pred_prob[:,1]})\n",
    "submit = pd.DataFrame()\n",
    "submit['id'] = test[\"id\"]\n",
    "submit['target'] = y_pred_prob[:,1]\n",
    "#submit.to_csv('lr_porto.csv.gz',index=False,compression='gzip') \n",
    "submit.to_csv('sumbit.csv',index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
