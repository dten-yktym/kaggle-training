{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 宣言部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:58:30.401884Z",
     "iopub.status.busy": "2022-12-09T01:58:30.401079Z",
     "iopub.status.idle": "2022-12-09T01:58:33.6412Z",
     "shell.execute_reply": "2022-12-09T01:58:33.640177Z",
     "shell.execute_reply.started": "2022-12-09T01:58:30.40179Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from numba import jit\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import SCORERS\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:58:33.645631Z",
     "iopub.status.busy": "2022-12-09T01:58:33.645251Z",
     "iopub.status.idle": "2022-12-09T01:58:33.957811Z",
     "shell.execute_reply": "2022-12-09T01:58:33.956509Z",
     "shell.execute_reply.started": "2022-12-09T01:58:33.64559Z"
    }
   },
   "outputs": [],
   "source": [
    "#計算時間算出\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        #thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        #tmin, tsec = divmod(temp_sec, 60)\n",
    "        #print('Time: %i H %i M %s sec' % (thour, tmin, round(tsec, 2)))\n",
    "        tsec = (datetime.now() - start_time).total_seconds()\n",
    "        print('Time:' + str(tsec))\n",
    "\n",
    "#gini係数算出\n",
    "@jit\n",
    "def gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', gini(labels, preds), True\n",
    "\n",
    "#未使用データ削除\n",
    "#'ps_car_03_cat', 'ps_car_05_cat'欠損値多過ぎる、'ps_car_11_cat'カテゴリにしては多過ぎる\n",
    "def dropmissingcol(pdData):\n",
    "    vars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat','ps_car_11_cat']\n",
    "    pdData.drop(vars_to_drop, inplace=True, axis=1)\n",
    "    return pdData\n",
    "\n",
    "#欠損値補完\n",
    "def missingvalues(pdData):\n",
    "    mean_imp = SimpleImputer(missing_values=-1, strategy='mean')\n",
    "    mode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')#catは最頻値\n",
    "    mode_col = ['ps_ind_02_cat','ps_ind_04_cat','ps_ind_05_cat',\n",
    "                'ps_car_01_cat','ps_car_02_cat','ps_car_07_cat','ps_car_09_cat']\n",
    "    pd_return = pdData.copy()\n",
    "    features = pdData.columns\n",
    "    for i in features:\n",
    "        if i in mode_col:\n",
    "            pd_return[i] = mode_imp.fit_transform(pdData[[i]]).ravel()\n",
    "        else:\n",
    "            pd_return[i] = mean_imp.fit_transform(pdData[[i]]).ravel()\n",
    "    return pd_return\n",
    "\n",
    "#カテゴリone-hot\n",
    "def encodecat(train, test):\n",
    "    cat_features = [col for col in train.columns if '_cat' in col]\n",
    "    for column in cat_features:\n",
    "        temp = pd.get_dummies(pd.Series(train[column]), prefix=column)\n",
    "        train = pd.concat([train, temp], axis=1)\n",
    "        train = train.drop([column], axis=1)\n",
    "\n",
    "    for column in cat_features:\n",
    "        temp = pd.get_dummies(pd.Series(test[column]), prefix=column)\n",
    "        test = pd.concat([test, temp], axis=1)\n",
    "        test = test.drop([column], axis=1)\n",
    "    return train, test\n",
    "\n",
    "#標準化\n",
    "def RescaleData(train, test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(train)\n",
    "    scaler.fit_transform(test)\n",
    "    return train, test\n",
    "\n",
    "#calc削除\n",
    "def DropCalcCol(train, test):\n",
    "    col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "    train = train.drop(col_to_drop, axis=1)\n",
    "    test = test.drop(col_to_drop, axis=1)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ読込・確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:58:33.959813Z",
     "iopub.status.busy": "2022-12-09T01:58:33.959331Z",
     "iopub.status.idle": "2022-12-09T01:58:56.561754Z",
     "shell.execute_reply": "2022-12-09T01:58:56.560545Z",
     "shell.execute_reply.started": "2022-12-09T01:58:33.959761Z"
    }
   },
   "outputs": [],
   "source": [
    "#データ読込\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データ確認\n",
    "x = np.shape(train)[0]\n",
    "y = np.shape(train)[1]\n",
    "train_columns = train.columns\n",
    "print('x = ' + str(x))\n",
    "print('y = ' + str(y))\n",
    "print(train_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各種データ分割　データ系統ごと\n",
    "train_target = train['target'].values\n",
    "train_id = train['id'].values\n",
    "test_id = test['id'].values\n",
    "train = dropmissingcol(train)\n",
    "test = dropmissingcol(test)\n",
    "train = train.drop(['id','target'], axis=1)\n",
    "test = test.drop(['id'], axis=1)\n",
    "\n",
    "train_cat_cols = []\n",
    "train_bin_cols = []\n",
    "train_num_cols = []\n",
    "for col in train.columns:\n",
    "    if '_cat' in col:\n",
    "        train_cat_cols.append(col)\n",
    "    elif '_bin' in col:\n",
    "        train_bin_cols.append(col)\n",
    "    else:\n",
    "        train_num_cols.append(col)\n",
    "\n",
    "train_cat = train[train_cat_cols]\n",
    "train_bin = train[train_bin_cols]\n",
    "train_num = train[train_num_cols]\n",
    "\n",
    "test_cat_cols = []\n",
    "test_bin_cols = []\n",
    "test_num_cols = []\n",
    "for col in test.columns:\n",
    "    if '_cat' in col:\n",
    "        test_cat_cols.append(col)\n",
    "    elif '_bin' in col:\n",
    "        test_bin_cols.append(col)\n",
    "    else:\n",
    "        test_num_cols.append(col)\n",
    "\n",
    "test_cat = test[test_cat_cols]\n",
    "test_bin = test[test_bin_cols]\n",
    "test_num = test[test_num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cat:' + train_cat.columns)\n",
    "print('bin:' + train_bin.columns)\n",
    "print('num:' + train_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各種データ分割　データ種類ごと\n",
    "#train = train.drop(['target', 'id'], axis=1)\n",
    "train_ind_cols = []\n",
    "train_reg_cols = []\n",
    "train_car_cols = []\n",
    "train_calc_cols = []\n",
    "for col in train.columns:\n",
    "    if 'ps_ind_' in col:\n",
    "        train_ind_cols.append(col)\n",
    "    elif 'ps_reg_' in col:\n",
    "        train_reg_cols.append(col)\n",
    "    elif 'ps_car_' in col:\n",
    "        train_car_cols.append(col)       \n",
    "    else:\n",
    "        train_calc_cols.append(col)\n",
    "\n",
    "train_ind = train[train_ind_cols]\n",
    "train_reg = train[train_reg_cols]\n",
    "train_car = train[train_car_cols]\n",
    "train_calc = train[train_calc_cols]\n",
    "\n",
    "test_ind_cols = []\n",
    "test_reg_cols = []\n",
    "test_car_cols = []\n",
    "test_calc_cols = []\n",
    "for col in test.columns:\n",
    "    if 'ps_ind_' in col:\n",
    "        test_ind_cols.append(col)\n",
    "    elif 'ps_reg_' in col:\n",
    "        test_reg_cols.append(col)\n",
    "    elif 'ps_car_' in col:\n",
    "        test_car_cols.append(col)       \n",
    "    else:\n",
    "        test_calc_cols.append(col)\n",
    "\n",
    "test_ind = test[test_ind_cols]\n",
    "test_reg = test[test_reg_cols]\n",
    "test_car = test[test_car_cols]\n",
    "test_calc = test[test_calc_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ind:' + train_ind.columns)\n",
    "print('reg:' + train_reg.columns)\n",
    "print('car:' + train_car.columns)\n",
    "print('calc:' + train_calc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainデータ選択\n",
    "#train_temp = train_cat\n",
    "#test_temp = test_cat\n",
    "train_temp =pd.concat([train_cat, train_num], axis=1)\n",
    "test_temp =pd.concat([test_cat, test_num], axis=1)\n",
    "print(train_temp.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ラベルデータ設定\n",
    "y_train = train_target\n",
    "\n",
    "#欠損値補完\n",
    "X = missingvalues(train_temp)\n",
    "X_test = missingvalues(test_temp)\n",
    "X = pd.DataFrame(X)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "#calc削除、on-hot、正規化\n",
    "X, X_test = encodecat(X, X_test)\n",
    "X, X_test = RescaleData(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:58:33.959813Z",
     "iopub.status.busy": "2022-12-09T01:58:33.959331Z",
     "iopub.status.idle": "2022-12-09T01:58:56.561754Z",
     "shell.execute_reply": "2022-12-09T01:58:56.560545Z",
     "shell.execute_reply.started": "2022-12-09T01:58:33.959761Z"
    }
   },
   "outputs": [],
   "source": [
    "#データ数削減\n",
    "train_num = 50000\n",
    "X = X.loc[:train_num - 1]\n",
    "y_train = np.delete(y_train,range(train_num,len(y_train)),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 複数手法で検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#データ分割\n",
    "X_train, X_val, Y_train, Y_val= train_test_split(X, y_train, test_size=0.10)\n",
    "\n",
    "#各モデルで検証\n",
    "Models = {\n",
    "    \"LR\":LinearRegression(),                                        #線形回帰モデル\n",
    "    \"KNNR\":KNeighborsRegressor(),                                   #k-近傍回帰\n",
    "    \"SVR\":SVR(),                                                    #サポートベクター回帰\n",
    "    \"DT\":DecisionTreeRegressor(),                                   #決定木回帰\n",
    "    \"RF\":RandomForestRegressor(n_estimators=625),                   #ランダムフォレスト回帰\n",
    "    \"GBR\":GradientBoostingRegressor(n_estimators=4000,alpha=0.01), #勾配ブースティング回帰\n",
    "    \"XGBR\":XGBRegressor(n_estimators=550),                         #XGBoost回帰\n",
    "    \"LGBM\":LGBMRegressor(n_estimators=1200, alpha=0.02)            #LightGMB回帰\n",
    "}\n",
    "\n",
    "for name, model in Models.items():\n",
    "    print(f\"Using Model: {name}\" )\n",
    "    start_time = timer(None)\n",
    "    model.fit(X_train, Y_train)\n",
    "    timer(start_time)\n",
    "    print(f'Training Score: {model.score(X_train, Y_train)}')\n",
    "    print(f'Test Score: {model.score(X_val, Y_val)}')\n",
    "    Validate_Predictions = model.predict(X_val)\n",
    "    print(f'gini: {gini(Y_val, Validate_Predictions)}')\n",
    "    print('-'*45)\n",
    "    #joblib.dump(model, name+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ラッパー法で特徴量選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データ分割\n",
    "X_train, X_val, Y_train, Y_val= train_test_split(X, y_train, test_size=0.10)\n",
    "\n",
    "feature = X_train.shape[1]\n",
    "print(feature)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#sfs = SFS(LinearRegression(),　　　　　　　　　　　　　　　　　　　#LR\n",
    "sfs = SFS(LGBMRegressor(n_estimators=1200, alpha=0.02),          #LGBM\n",
    "           k_features=feature,\n",
    "           forward=True,\n",
    "           floating=False,\n",
    "           scoring = 'neg_mean_squared_error',\n",
    "           cv = skf)\n",
    "sfs.fit(X_train,Y_train)\n",
    "df_SFS_results = pd.DataFrame(sfs.subsets_).transpose()\n",
    "df_SFS_results['avg_score'] = df_SFS_results[\"avg_score\"].astype(float)\n",
    "df_SFS_results\n",
    "#df_SFS_results.to_csv('df_SFS_results_LGBM.csv') #結果保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Searchでパラメータ最適化　LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "#paramaters to search over\n",
    "params = {\n",
    "    'min_child_weight': [5, 10, 12, 15, 30, 50, 100, 150],\n",
    "    'num_leaves': [4, 5, 8, 10, 15, 20, 30],\n",
    "    'subsample': [0.2, 0.4, 0.6, 0.8],\n",
    "    'drop_rate': [0.1, 0.3, 0.5, 0.7, 0.15, 0.2],\n",
    "    'max_depth': [3, 4, 5, 7, 10, 12, 15, 20]\n",
    "}\n",
    "#classifier model\n",
    "model = lgbm.LGBMClassifier(learning_rate=LEARNING_RATE, n_estimators=600, objective='binary', )\n",
    "\n",
    "#folds to use in stratified k-fold\n",
    "folds = 3\n",
    "#how many combinations of the above parameters should we try\n",
    "param_comb = 10\n",
    "#the algorithm is going to run folds x param_comb times\n",
    "\n",
    "SKfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "#set up search with SKfold split\n",
    "random_search = RandomizedSearchCV(model, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4,\n",
    "                                   cv=SKfold.split(X, y_train), verbose=3, random_state=1)\n",
    "\n",
    "\n",
    "#UNCOMMENT FOLLOWING TO RUN GRIDSEARCH\n",
    "start_time = timer(None)\n",
    "#start search\n",
    "random_search.fit(X, y_train)\n",
    "timer(start_time)\n",
    "\n",
    "print('All results:')\n",
    "print(random_search.cv_results_)\n",
    "print('Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('Best Normalised gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(random_search.best_score_)\n",
    "print('Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results.to_csv('lightgbm-randomgridsearch-results-03.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ベイズでパラメータ最適化　LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(num_leaves, min_child_weight, feature_fraction, subsample, drop_rate, max_depth):\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"learning_rate\": 0.07,\n",
    "        \"verbosity\": -1,\n",
    "        \"num_leaves\": int(num_leaves),\n",
    "        \"min_child_weight\": min_child_weight,\n",
    "        \"feature_fraction\": feature_fraction,\n",
    "        \"subsample\": subsample,\n",
    "        'drop_rate': drop_rate,\n",
    "        'max_depth': int(max_depth)\n",
    "    }\n",
    "    num_boost_round = 10000\n",
    "\n",
    "    # define the number of folds for cross-validation\n",
    "    n_folds = 5\n",
    "\n",
    "    # create a stratified k-fold iterator\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=1)\n",
    "\n",
    "    # initialize a list to store the evaluation metric for each fold\n",
    "    scores = []\n",
    "\n",
    "    # iterate over the folds\n",
    "    for id_train, id_val in skf.split(X, y_train):\n",
    "        # get the training and validation data for this fold\n",
    "        X_train_fold = X.iloc[id_train]\n",
    "        y_train_fold = y_train[id_train]\n",
    "        X_val_fold = X.iloc[id_val]\n",
    "        y_val_fold = y_train[id_val]\n",
    "\n",
    "        lgb_train = lgbm.Dataset(X_train_fold, y_train_fold)\n",
    "        lgb_val = lgbm.Dataset(X_val_fold, y_val_fold)\n",
    "\n",
    "        # train the model with the specified parameters on the training data\n",
    "        model = lgbm.train(params, lgb_train, num_boost_round, valid_sets=lgb_val, feval=evalerror, verbose_eval=False,\n",
    "                           early_stopping_rounds=100)\n",
    "        scores.append(model.best_score['valid_0']['gini'])\n",
    "\n",
    "    # return the mean evaluation metric across all folds\n",
    "    return np.mean(scores)\n",
    "\n",
    "# define the hyperparameters to be optimised\n",
    "hyperparameters = {\n",
    "    \"num_leaves\": (4, 50),\n",
    "    \"min_child_weight\": (0.001, 150),\n",
    "    \"feature_fraction\": (0.1, 0.9),\n",
    "    \"subsample\": (0.1, 1),\n",
    "    'drop_rate': (0.1, 0.8),\n",
    "    'max_depth': (3, 20)\n",
    "}\n",
    "\n",
    "#UNCOMMENT THE FOLLOWING TO RUN BAYESIAN OPTIMISATION\n",
    "\n",
    "# perform Bayesian optimisation to find the optimal hyperparameters\n",
    "optimizer = BayesianOptimization(evaluate_model, hyperparameters)\n",
    "optimizer.maximize(n_iter=10)\n",
    "\n",
    "# display the optimal values of the hyperparameters\n",
    "print(\"Optimal hyperparameters:\")\n",
    "print(optimizer.max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM modelで学習、提出データ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T01:58:56.616769Z",
     "iopub.status.busy": "2022-12-09T01:58:56.615959Z",
     "iopub.status.idle": "2022-12-09T02:05:55.323959Z",
     "shell.execute_reply": "2022-12-09T02:05:55.322747Z",
     "shell.execute_reply.started": "2022-12-09T01:58:56.616726Z"
    }
   },
   "outputs": [],
   "source": [
    "#パラメータ設定\n",
    "min_data_in_leaf = 2000\n",
    "num_boost_round = 10000\n",
    "params = {'n_estimators': 1200,\n",
    "          'alpha': 0.02\n",
    "          }\n",
    "\n",
    "#交差検証\n",
    "folds = 5\n",
    "SKfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "#スコア格納のための空変数\n",
    "best_trees = []\n",
    "fold_scores = []\n",
    "\n",
    "cv_train = np.zeros(len(y_train))\n",
    "cv_pred = np.zeros(len(X_test))\n",
    "\n",
    "start_time = timer(None)\n",
    "#過学習防止のため平均\n",
    "iterations = 3\n",
    "for seed in range(iterations):\n",
    "    timer(start_time)\n",
    "    params['seed'] = seed\n",
    "    #交差検証開始\n",
    "    for id_train, id_test in SKfold.split(X, y_train):\n",
    "        #x train, x validation\n",
    "        xtr, xvl = X.loc[id_train], X.loc[id_test]\n",
    "        #y train, y validation\n",
    "        ytr, yvl = y_train[id_train], y_train[id_test]\n",
    "        #efficient datastructures for lgbm\n",
    "        dtrain = lgbm.Dataset(data=xtr, label=ytr)\n",
    "        dval = lgbm.Dataset(data=xvl, label=yvl, reference=dtrain)\n",
    "        #学習モデル作成\n",
    "        bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dval, feval=evalerror, verbose_eval=100,\n",
    "                         early_stopping_rounds=100)\n",
    "        #ベストを保存\n",
    "        best_trees.append(bst.best_iteration)\n",
    "        fold_scores.append(bst.best_score)\n",
    "        #フォールドベストを学習\n",
    "        cv_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "pd.DataFrame({'id': test_id, 'target': cv_pred / (iterations * folds)}).to_csv('submission_LGBM.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear modelで学習、提出データ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#パラメータ設定\n",
    "Lr=LinearRegression()\n",
    "params = ()\n",
    "\n",
    "#交差検証\n",
    "folds = 5\n",
    "SKfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "#スコア格納のための空変数\n",
    "best_trees = []\n",
    "fold_scores = []\n",
    "\n",
    "cv_train = np.zeros(len(y_train))\n",
    "cv_pred = np.zeros(len(X_test))\n",
    "\n",
    "fold_count = 0\n",
    "#交差検証開始\n",
    "for id_train, id_test in SKfold.split(X, y_train):\n",
    "    fold_count += 1\n",
    "    #x train, x validation\n",
    "    xtr, xvl = X.loc[id_train], X.loc[id_test]\n",
    "    #y train, y validation\n",
    "    ytr, yvl = y_train[id_train], y_train[id_test]\n",
    "    #学習モデル作成\n",
    "    bst = Lr.fit(xtr,ytr)\n",
    "    bst_val = bst.predict(xvl)\n",
    "    bst_val[bst_val < 0] = 0\n",
    "    fold_gini = gini(yvl,bst_val)\n",
    "    print('fold' + str(fold_count) + ':' + str(fold_gini))\n",
    "    test_pred = bst.predict(X_test)\n",
    "    test_pred[test_pred < 0] = 0\n",
    "    cv_pred += test_pred\n",
    "    \n",
    "pd.DataFrame({'id': test_id, 'target': cv_pred / (folds)}).to_csv('submission_linear.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
